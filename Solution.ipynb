import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic time series data for stock prices
np.random.seed(0)
num_samples = 1000
time = np.arange(num_samples)
trend = 0.1 * time  # linear increasing trend
seasonality = 10 * np.sin(2 * np.pi * time / 50)  # periodic seasonality
noise = np.random.normal(loc=0, scale=1, size=num_samples)  # random noise
stock_prices = trend + seasonality + noise

# Create a pandas DataFrame
df = pd.DataFrame({'Time': time, 'StockPrice': stock_prices})

# Plot the synthetic time series data
plt.figure(figsize=(10, 5))
plt.plot(df['Time'], df['StockPrice'], label='Stock Price')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.title('Synthetic Time Series Data')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(df['StockPrice'].values.reshape(-1, 1))

# Split the dataset into training and test sets
train_size = int(len(scaled_data) * 0.8)
test_size = len(scaled_data) - train_size
train_data, test_data = scaled_data[0:train_size, :], scaled_data[train_size:len(scaled_data), :]

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout

def create_lstm_model(units, num_layers, dropout_rate):
    model = Sequential()
    for i in range(num_layers - 1):
        model.add(LSTM(units, return_sequences=True, input_shape=(1, 1)))
        model.add(Dropout(dropout_rate))
    model.add(LSTM(units))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1))
    return model

# Define hyperparameters
units = 50
num_layers = 2
dropout_rate = 0.2

# Create the LSTM model
model = create_lstm_model(units, num_layers, dropout_rate)

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam')

# Reshape the data for LSTM input [samples, time steps, features]
def create_dataset(dataset, time_steps=1):
    X, Y = [], []
    for i in range(len(dataset) - time_steps):
        X.append(dataset[i:(i + time_steps), 0])
        Y.append(dataset[i + time_steps, 0])
    return np.array(X), np.array(Y)

time_steps = 1
X_train, y_train = create_dataset(train_data, time_steps)
X_test, y_test = create_dataset(test_data, time_steps)

# Reshape input to be [samples, time steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], time_steps, 1))
X_test = np.reshape(X_test, (X_test.shape[0], time_steps, 1))

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), verbose=1)

# Make predictions
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Inverse transform predictions
train_predict = scaler.inverse_transform(train_predict)
y_train = scaler.inverse_transform([y_train])
test_predict = scaler.inverse_transform(test_predict)
y_test = scaler.inverse_transform([y_test])

# Calculate RMSE
from sklearn.metrics import mean_squared_error
train_rmse = np.sqrt(mean_squared_error(y_train[0], train_predict[:,0]))
test_rmse = np.sqrt(mean_squared_error(y_test[0], test_predict[:,0]))

print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(df['Time'][:len(train_predict)], y_train.reshape(-1), label='Train True')
plt.plot(df['Time'][len(train_predict):], y_test.reshape(-1), label='Test True')
plt.plot(df['Time'][:len(train_predict)], train_predict.reshape(-1), label='Train Predict')
plt.plot(df['Time'][len(train_predict):], test_predict.reshape(-1), label='Test Predict')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.title('Model Predictions vs True Values')
plt.legend()
plt.grid(True)
plt.show()

from keras.optimizers import Adam

# Define a function to create and compile the LSTM model
def create_compile_lstm_model(units, num_layers, dropout_rate, learning_rate):
    model = Sequential()
    for i in range(num_layers - 1):
        model.add(LSTM(units, return_sequences=True, input_shape=(1, 1)))
        model.add(Dropout(dropout_rate))
    model.add(LSTM(units))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1))
    
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(loss='mean_squared_error', optimizer=optimizer)
    
    return model

# Define hyperparameter ranges for tuning
learning_rates = [0.001, 0.01, 0.1]
batch_sizes = [32, 64, 128]

best_rmse = float('inf')
best_learning_rate = None
best_batch_size = None

# Hyperparameter tuning loop
for lr in learning_rates:
    for batch_size in batch_sizes:
        print(f"Training with learning rate: {lr}, batch size: {batch_size}")
        
        # Create and compile the model with current hyperparameters
        model = create_compile_lstm_model(units, num_layers, dropout_rate, lr)
        
        # Train the model
        history = model.fit(X_train, y_train, epochs=100, batch_size=batch_size, 
                            validation_data=(X_test, y_test), verbose=0)
        
        # Make predictions
        train_predict = model.predict(X_train)
        test_predict = model.predict(X_test)
        
        # Inverse transform predictions
        train_predict = scaler.inverse_transform(train_predict)
        y_train_inv = scaler.inverse_transform([y_train])
        test_predict = scaler.inverse_transform(test_predict)
        y_test_inv = scaler.inverse_transform([y_test])
        
